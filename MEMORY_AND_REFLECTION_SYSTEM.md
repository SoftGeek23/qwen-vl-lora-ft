# Memory and Reflection System Documentation

## Overview

The agent implements a two-phase learning system: an **active execution phase** where the agent performs tasks, and a **sleep state reflection phase** where a specialized reflector agent analyzes completed task logs to extract insights. This system uses semantic embeddings to enable context-aware retrieval of both specific memories and high-level strategies, allowing the agent to learn from past experiences and apply relevant guidance dynamically.

## Reflector Agent and Sleep State

The **ReflectorAgent** operates in a "sleep state" that activates after task execution completes. When `harness.run()` finishes executing a batch of tasks, it calls `_reflect_and_update_memory()` which collects all task logs from the results directory. The reflector analyzes these logs using either LLM-based reflection (configured via `reflector_model`, e.g., "sonnet-3.7:thinking") or rule-based templates as a fallback. The reflector identifies common failure patterns, successful strategies, and cross-task insights, then generates two types of outputs: **MemoryExemplars** (specific state-action-result-reflection tuples) and **HighLevelStrategies** (generalizable best practices). These insights are stored persistently: memories in `memories.jsonl` (one JSON object per line) and strategies in `strategies.json` (a JSON array). The sleep state pattern ensures reflection happens asynchronously without blocking task execution, and new insights are immediately available for subsequent runs.

## Semantic Embeddings and Memory Retrieval

The system uses **sentence-transformers** (default model: `BAAI/bge-large-en-v1.5`) to create semantic embeddings for both memories and strategies. When memories are added, each memory is embedded as a concatenation of its state summary, action, result context (e.g., "Failure: ..." or "Success"), and reflection, creating a rich representation that captures both the situation and the lesson learned. During agent execution, the current state summary (derived from the page's accessibility tree, URL, and visible elements) is embedded and compared against all stored memory embeddings using cosine similarity. The top-k most similar memories (default: 3) are retrieved and injected into the user messages as formatted text, providing the agent with relevant past experiences. This semantic retrieval enables the agent to find relevant memories even when the exact state differs, as long as the semantic context is similarâ€”for example, retrieving a memory about dropdown interactions when encountering a similar dropdown scenario, even if the element IDs differ.

## Strategy Management and Current Implementation

High-level strategies are stored in `strategies.json` and managed by the `StrategyManager` class. Currently, **all strategies are injected into the system prompt** on every turn, which can overwhelm the model when the strategy count grows large (currently 32 strategies). Strategies are automatically consolidated and reordered: the `consolidate_and_reorder()` method removes duplicate "Strategy X:" prefixes from LLM-generated strategies, deduplicates exact matches, prioritizes critical strategies (those mentioning "one action per turn", "multi-action", etc.) to the top of the list, and removes substring duplicates. Strategies are formatted as a numbered list in the system prompt under the heading "## High-Level Strategies and Best Practices", ensuring they are always available to guide the agent's decision-making.

## Proposed Improvement: Smart Strategy Retrieval

To reduce prompt bloat and improve relevance, the system should implement **semantic strategy retrieval** similar to memory retrieval. Instead of injecting all 32 strategies, the agent would retrieve only the **top 5-7 most relevant strategies** based on the current state context. This would be implemented by: (1) embedding all strategies using the same sentence-transformer model used for memories, (2) computing cosine similarity between the current state summary (and optionally current error/action context) and all strategy embeddings, (3) retrieving the top-k most relevant strategies, and (4) always including critical strategies (e.g., "one action per turn") regardless of similarity score to ensure fundamental rules are never omitted. The `StrategyManager.get_strategies_for_prompt()` method would be modified to accept optional `current_state_summary`, `current_error`, and `top_k` parameters, defaulting to semantic retrieval when context is provided and falling back to all strategies when not (for backward compatibility). This approach mirrors the memory retrieval pattern, maintains consistency in the codebase, and significantly reduces prompt size while ensuring the most relevant guidance is provided at each step.

